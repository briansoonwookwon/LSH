{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Matrix Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import hashlib\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from utils.utils import clean_document, shingle, minhash\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def generate_shingles(text, k=2):\n",
    "    \"\"\"Generate k-shingles (substrings of length k) from the text.\"\"\"\n",
    "    split = text.split()\n",
    "    return {' '.join(split[i:i+k]) for i in range(len(split) - k + 1)}\n",
    "\n",
    "def create_characteristic_matrix(shingle_dict, docs):\n",
    "    \"\"\"Create a binary characteristic matrix based on shingles.\"\"\"\n",
    "    all_shingles = list(set(shingle for doc_shingles in shingle_dict.values() for shingle in doc_shingles))\n",
    "    matrix = np.zeros((len(all_shingles), len(docs)), dtype=int)\n",
    "    \n",
    "    shingle_index = {shingle: idx for idx, shingle in enumerate(all_shingles)}\n",
    "    \n",
    "    doc_index_mapping = {doc_id: idx for idx, doc_id in enumerate(docs.keys())}  # Mapping of original doc_id to matrix index\n",
    "\n",
    "    for doc_id, doc in enumerate(docs):\n",
    "        for shingle in shingle_dict[doc]:\n",
    "            matrix[shingle_index[shingle], doc_id] = 1\n",
    "\n",
    "    return matrix, all_shingles, doc_index_mapping\n",
    "\n",
    "def hash_shingle(shingle, seed):\n",
    "    \"\"\"Hash function for shingle using a seed.\"\"\"\n",
    "    return int(hashlib.md5((shingle + str(seed)).encode()).hexdigest(), 16)\n",
    "\n",
    "def minhash_signature(characteristic_matrix, num_hashes, all_shingles):\n",
    "    \"\"\"Generate Minhash signatures for each document.\"\"\"\n",
    "    num_shingles, num_docs = characteristic_matrix.shape\n",
    "    signatures = np.full((num_hashes, num_docs), np.inf)  # Initialize signatures with infinity\n",
    "    \n",
    "    # Apply multiple hash functions\n",
    "    for i in range(num_hashes):\n",
    "        # Hash function: apply a different hash function for each iteration\n",
    "        hash_values = np.array([hash_shingle(shingle, i) for shingle in all_shingles])  # Hash shingles\n",
    "        # hash_values = np.array(Parallel(n_jobs=-1)(delayed(hash_shingle)(shingle, i) for shingle in all_shingles))\n",
    "        for doc_idx in range(num_docs):\n",
    "            # For each document, find the minimum hash value of the shingles that are present (value 1)\n",
    "            signatures[i, doc_idx] = np.min(hash_values[characteristic_matrix[:, doc_idx] == 1])\n",
    "\n",
    "    return signatures\n",
    "\n",
    "def lsh(signatures, bands, rows, doc_index_mapping):\n",
    "    \"\"\"Apply Locality Sensitive Hashing (LSH) to group similar signatures.\"\"\"\n",
    "    assert bands * rows == signatures.shape[0], \"Number of hash functions must equal bands * rows\"\n",
    "    \n",
    "    candidate_pairs = set()\n",
    "    buckets = defaultdict(list)\n",
    "    \n",
    "    for b in range(bands):\n",
    "        for doc_idx in range(signatures.shape[1]):\n",
    "            band = tuple(signatures[b*rows:(b+1)*rows, doc_idx])\n",
    "            buckets[band].append(doc_idx)\n",
    "        \n",
    "        # Collect candidate pairs from the buckets\n",
    "        for bucket in buckets.values():\n",
    "            if len(bucket) > 1:\n",
    "                for i in range(len(bucket)):\n",
    "                    for j in range(i + 1, len(bucket)):\n",
    "                        doc1 = list(doc_index_mapping.keys())[list(doc_index_mapping.values()).index(bucket[i])]\n",
    "                        doc2 = list(doc_index_mapping.keys())[list(doc_index_mapping.values()).index(bucket[j])]\n",
    "                        candidate_pairs.add((doc1, doc2))\n",
    "        buckets.clear()\n",
    "    \n",
    "    return candidate_pairs\n",
    "\n",
    "\n",
    "def compute_hash_values(i, all_shingles):\n",
    "    \"\"\"Compute hash values for a specific hash function (i) across all shingles.\"\"\"\n",
    "    return i, np.array([int(hashlib.md5((str(s) + str(i)).encode()).hexdigest(), 16) for s in all_shingles])\n",
    "\n",
    "from utils.utils import read_tsv\n",
    "\n",
    "num_hashes = 100\n",
    "bands = 20\n",
    "rows_per_band = num_hashes // bands\n",
    "\n",
    "tsv_dict = read_tsv('../../data/hundred.tsv')\n",
    "\n",
    "seen_docs = {}\n",
    "unique_docs = {}\n",
    "cleaned_docs = {}\n",
    "exact_duplicates = {}\n",
    "\n",
    "for doc_id, doc in tsv_dict.items():\n",
    "    unique_docs[doc_id] = doc\n",
    "    cleaned_docs[doc_id] = clean_document(doc)\n",
    "\n",
    "shingle_dict = {doc: generate_shingles(text, k=5) for doc, text in cleaned_docs.items()}\n",
    "characteristic_matrix, all_shingles, doc_index_mapping = create_characteristic_matrix(shingle_dict, cleaned_docs)\n",
    "\n",
    "seen_docs = {}\n",
    "unique_docs = {}\n",
    "exact_duplicates = {}\n",
    "\n",
    "for doc_id, doc in tsv_dict.items():\n",
    "    if doc not in seen_docs:\n",
    "        unique_docs[doc_id] = doc\n",
    "        seen_docs[doc] = doc_id  # Track first occurrence of the document\n",
    "    else:\n",
    "        original_id = seen_docs[doc]\n",
    "        if original_id not in exact_duplicates:\n",
    "            exact_duplicates[original_id] = []\n",
    "        exact_duplicates[original_id].append(doc_id)\n",
    "\n",
    "cleaned_docs = {doc_id: clean_document(doc) for doc_id, doc in unique_docs.items()}\n",
    "shingle_dict = {doc: generate_shingles(text, k=5) for doc, text in cleaned_docs.items()}\n",
    "characteristic_matrix, all_shingles, doc_index_mapping = create_characteristic_matrix(shingle_dict, cleaned_docs)\n",
    "\n",
    "num_shingles, num_docs = characteristic_matrix.shape\n",
    "signatures = np.full((num_hashes, num_docs), np.inf)\n",
    "\n",
    "# Compute hash values for minhash signatures\n",
    "hash_results = Parallel(n_jobs=-1)(delayed(compute_hash_values)(i, all_shingles) for i in range(num_hashes))\n",
    "\n",
    "# Reconstruct the hash dictionary from the results\n",
    "hash_dict = {i: hashes for i, hashes in hash_results}\n",
    "\n",
    "# Apply Minhashing\n",
    "signatures = np.full((num_hashes, len(unique_docs)), np.inf)\n",
    "for i in range(num_hashes):\n",
    "    hash_values = hash_dict[i]\n",
    "    for doc_idx in range(len(unique_docs)):\n",
    "        present_shingles = np.where(characteristic_matrix[:, doc_idx] == 1)[0]\n",
    "        if len(present_shingles) > 0:\n",
    "            signatures[i, doc_idx] = np.min(hash_values[present_shingles])\n",
    "\n",
    "# Apply LSH on the signatures\n",
    "candidate_pairs = lsh(signatures, bands, rows_per_band, doc_index_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import UnionFind\n",
    "# Perform Union-Find to group similar documents\n",
    "uf = UnionFind()\n",
    "for doc1, doc2 in candidate_pairs:\n",
    "    uf.union(doc1, doc2)\n",
    "\n",
    "# Group documents by their root in Union-Find\n",
    "clusters = defaultdict(list)\n",
    "for doc_id in unique_docs:\n",
    "    root = uf.find(doc_id)\n",
    "    clusters[root].append(doc_id)\n",
    "\n",
    "# Add the exact duplicates to their respective clusters\n",
    "for original_doc, duplicates in exact_duplicates.items():\n",
    "    root = uf.find(original_doc)\n",
    "    clusters[root].extend(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7 19 42 49 51 77 87 93\n",
      "8\n",
      "9\n",
      "10 22 85\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16 26 34 48 57 72 75 89 98\n",
      "17\n",
      "18\n",
      "20\n",
      "21 33 80 86 96\n",
      "23\n",
      "24\n",
      "25 32\n",
      "27\n",
      "28 37 97\n",
      "29 31 35 36 40 62 67 68 95 100\n",
      "30\n",
      "38 53 99\n",
      "39 94\n",
      "41\n",
      "43\n",
      "44\n",
      "45\n",
      "46 76\n",
      "47\n",
      "50\n",
      "52\n",
      "54 69\n",
      "55\n",
      "56\n",
      "58\n",
      "59\n",
      "60\n",
      "61 81\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "70\n",
      "71\n",
      "73\n",
      "74\n",
      "78\n",
      "79\n",
      "82\n",
      "83\n",
      "84\n",
      "88\n",
      "90\n",
      "91\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "for cluster_id, doc_ids in clusters.items():\n",
    "    # Join the doc_ids with spaces and write to the file\n",
    "    doc_ids_str = ' '.join(map(str, doc_ids))\n",
    "    print(doc_ids_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemented Base LSH in project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import re\n",
    "\n",
    "# Helper functions\n",
    "def clean_document(text):\n",
    "    \"\"\"Clean and normalize the document by lowercasing and removing special characters.\"\"\"\n",
    "    text = re.sub(r'[^a-z\\s]', '', text.lower())\n",
    "    return text\n",
    "\n",
    "def shingle(text, k=5):\n",
    "    \"\"\"Generate k-shingles from a given text.\"\"\"\n",
    "    split = text.split()\n",
    "    return {' '.join(split[i:i+k]) for i in range(len(split) - k + 1)}\n",
    "\n",
    "def minhash(shingles, num_hashes=100):\n",
    "    \"\"\"Generate minhash signature for the shingles.\"\"\"\n",
    "    signature = []\n",
    "    for i in range(num_hashes):\n",
    "        hash_vals = [int(hashlib.md5((str(s) + str(i)).encode()).hexdigest(), 16) for s in shingles]\n",
    "        signature.append(min(hash_vals))\n",
    "    return signature\n",
    "\n",
    "class UnionFind:\n",
    "    \"\"\"Union-Find (Disjoint Set) implementation with path compression.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.parent = {}\n",
    "\n",
    "    def find(self, x):\n",
    "        if x not in self.parent:\n",
    "            self.parent[x] = x\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])  # Path compression\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x, y):\n",
    "        rootX = self.find(x)\n",
    "        rootY = self.find(y)\n",
    "        if rootX != rootY:\n",
    "            self.parent[rootX] = rootY\n",
    "\n",
    "def read_tsv(tsv):\n",
    "    with open(tsv, 'r', encoding='utf-8') as file:\n",
    "        tsv_dict = {}\n",
    "        for line in file:\n",
    "            if line.strip():  # To skip empty lines\n",
    "                index, text = line.split('\\t', 1)\n",
    "                tsv_dict[int(index)] = text\n",
    "    return tsv_dict\n",
    "\n",
    "import hashlib\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from utils.utils import clean_document, shingle, minhash\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class LSH:\n",
    "    \"\"\"Locality Sensitive Hashing with MinHash and Banding.\"\"\"\n",
    "    def __init__(self, num_hashes=100, num_bands=20, rows_per_band=5, k=5):\n",
    "        self.num_hashes = num_hashes\n",
    "        self.num_bands = num_bands\n",
    "        self.rows_per_band = rows_per_band\n",
    "        self.index = defaultdict(list)\n",
    "        self.unique_docs = {}\n",
    "        self.cleaned_docs = {}\n",
    "        self.candidate_pairs = set()\n",
    "        self.exact_duplicates = {}\n",
    "\n",
    "        self.k = k\n",
    "        assert self.num_hashes == self.num_bands * self.rows_per_band, \"Hash functions must equal bands * rows_per_band\"\n",
    "    \n",
    "    def remove_duplicates(self, docs):\n",
    "        seen_docs = {}\n",
    "        for doc_id, doc in docs.items():\n",
    "            if doc not in seen_docs:\n",
    "                self.unique_docs[doc_id] = doc\n",
    "                seen_docs[doc] = doc_id  # Track first occurrence of the document\n",
    "            else:\n",
    "                # Track exact duplicates\n",
    "                original_id = seen_docs[doc]\n",
    "                if original_id not in self.exact_duplicates:\n",
    "                    self.exact_duplicates[original_id] = []\n",
    "                self.exact_duplicates[original_id].append(doc_id)\n",
    "\n",
    "    # def compute_minhash_signatures(self, docs):\n",
    "    #     \"\"\"Compute MinHash signatures for each document.\"\"\"\n",
    "    #     self.remove_duplicates(docs)\n",
    "    #     self.cleaned_docs = {doc_id: clean_document(doc) for doc_id, doc in self.unique_docs.items()}\n",
    "\n",
    "    #     self.shingle_sets = {doc_id: shingle(doc, self.k) for doc_id, doc in self.cleaned_docs.items()}\n",
    "    #     self.signatures = {doc_id: minhash(shingles, self.num_hashes) for doc_id, shingles in self.shingle_sets.items()}\n",
    "\n",
    "    #     return self.signatures\n",
    "\n",
    "\n",
    "    def compute_minhash_signatures(self, docs):\n",
    "        \"\"\"Compute MinHash signatures for each document in parallel.\"\"\"\n",
    "        self.remove_duplicates(docs)\n",
    "        self.cleaned_docs = {doc_id: clean_document(doc) for doc_id, doc in self.unique_docs.items()}\n",
    "        self.shingle_sets = {doc_id: shingle(doc, self.k) for doc_id, doc in self.cleaned_docs.items()}\n",
    "        \n",
    "        # Parallel computation of MinHash signatures\n",
    "        signatures = Parallel(n_jobs=-1)( delayed(minhash)(shingles, self.num_hashes) for doc_id, shingles in self.shingle_sets.items())\n",
    "        self.signatures = dict(zip(self.shingle_sets.keys(), signatures))\n",
    "\n",
    "        return self.signatures\n",
    "\n",
    "    def banding(self, signatures):\n",
    "        \"\"\"Apply LSH banding technique to find candidate pairs.\"\"\"\n",
    "        # Split the signature into bands\n",
    "        for doc_id, sig in signatures.items():\n",
    "            for band_idx in range(self.num_bands):\n",
    "                start = band_idx * self.rows_per_band\n",
    "                band = tuple(sig[start:start + self.rows_per_band])  # Use this band as the hash key\n",
    "                self.index[(band_idx, band)].append(doc_id)\n",
    "        \n",
    "        # Find candidate pairs from documents that share the same band\n",
    "        for doc_ids in self.index.values():\n",
    "            if len(doc_ids) > 1:\n",
    "                self.candidate_pairs.update(combinations(doc_ids, 2))  # All combinations of doc_ids in the same bucket\n",
    "        return self.candidate_pairs\n",
    "    \n",
    "    \n",
    "from collections import defaultdict\n",
    "from utils.utils import UnionFind, clean_document, shingle, minhash\n",
    "\n",
    "#Use Case 1\n",
    "def collection_deduplication(lsh):\n",
    "    # Step 5: Use Union-Find to cluster documents\n",
    "    uf = UnionFind()\n",
    "    for doc1, doc2 in lsh.candidate_pairs:\n",
    "        uf.union(doc1, doc2)\n",
    "    \n",
    "    # Group documents by their root in Union-Find\n",
    "    clusters = defaultdict(list)\n",
    "    for doc_id in lsh.unique_docs:\n",
    "        root = uf.find(doc_id)\n",
    "        clusters[root].append(doc_id)\n",
    "    \n",
    "    # Now include the exact duplicates\n",
    "    for original_id, duplicate_ids in lsh.exact_duplicates.items():\n",
    "        root = uf.find(original_id)\n",
    "        clusters[root].extend(duplicate_ids)  # Add the duplicates to the cluster of their original doc\n",
    "\n",
    "    return clusters\n",
    "\n",
    "# Use Case 2\n",
    "def nearest_neighbor_search(query_doc, lsh):\n",
    "    \"\"\"Find approximate nearest neighbors for a given query document.\"\"\"\n",
    "    query_doc_cleaned = clean_document(query_doc)\n",
    "    query_shingles = shingle(query_doc_cleaned, lsh.k)\n",
    "    query_signature = minhash(query_shingles, lsh.num_hashes)\n",
    "    \n",
    "    candidate_pairs = set()\n",
    "    # Find candidate pairs from the index\n",
    "    for band_idx in range(lsh.num_bands):\n",
    "        start = band_idx * lsh.rows_per_band\n",
    "        band = tuple(query_signature[start:start + lsh.rows_per_band])\n",
    "        if (band_idx, band) in lsh.index:\n",
    "            candidate_pairs.update(lsh.index[(band_idx, band)])\n",
    "    \n",
    "    return candidate_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7 19 42 49 51 77 87 93\n",
      "8\n",
      "9\n",
      "10 22 85\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16 26 34 48 57 72 75 89 98\n",
      "17\n",
      "18\n",
      "20\n",
      "21 33 80 86 96\n",
      "23\n",
      "24\n",
      "25 32\n",
      "27\n",
      "28 37 97\n",
      "29 31 35 36 40 62 67 68 95 100\n",
      "30\n",
      "38 53 99\n",
      "39 94\n",
      "41\n",
      "43\n",
      "44\n",
      "45\n",
      "46 76\n",
      "47\n",
      "50\n",
      "52\n",
      "54 69\n",
      "55\n",
      "56\n",
      "58\n",
      "59\n",
      "60\n",
      "61 81\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "70\n",
      "71\n",
      "73\n",
      "74\n",
      "78\n",
      "79\n",
      "82\n",
      "83\n",
      "84\n",
      "88\n",
      "90\n",
      "91\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "lsh = LSH(num_hashes=100, num_bands=20, rows_per_band=5, k=3)\n",
    "\n",
    "signatures = lsh.compute_minhash_signatures(tsv_dict)\n",
    "\n",
    "lsh.banding(signatures)\n",
    "\n",
    "clusters = collection_deduplication(lsh)\n",
    "\n",
    "for cluster_id, doc_ids in clusters.items():\n",
    "    # Join the doc_ids with spaces and write to the file\n",
    "    doc_ids_str = ' '.join(map(str, doc_ids))\n",
    "    print(doc_ids_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
